{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fed6147-755d-461a-8a56-75e6aaab935b",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Welcome to FUAM Deployment\n",
    "\n",
    "This notebook deployes the latest FUAM version in the specified workspace. It works for initial deployment and for the upgrade process of FUAM.\n",
    "\n",
    "**End-to-end documenation on fabric-toolbox:**\n",
    "\n",
    "[Visit - How to deploy and configure FUAM](https://github.com/microsoft/fabric-toolbox/blob/main/monitoring/fabric-unified-admin-monitoring/how-to/How_to_deploy_FUAM.md)\n",
    "\n",
    "**What is happening in this notebook?**\n",
    " - The notebook checks the two cloud connections for FUAM (if initial deployment, connections will be created, otherwise check only)\n",
    " - It downloads the latest FUAM src files from Github\n",
    " - It deploys/updates the Fabric items in the current workspace\n",
    " - It creates all needed tables automatically, so reports work also with some data missing\n",
    "\n",
    "**Next steps**\n",
    "- (Optional) Change connection names, only if needed\n",
    "- Run this notebook\n",
    "\n",
    "If you **deploy** FUAM in this workspace at the **first time**:\n",
    "- Navigate to the cloud connections\n",
    "- Search under cloud connection for **fuam fabric-service-api admin** and for **fuam pbi-service-api admin** \n",
    "- Add the credentials of your service principal to these connections\n",
    "\n",
    "If you **update** your existing FUAM workspace:\n",
    "- After the notebooks has been executed, you are **done**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439a740-1fc5-4e3c-a47e-de324036912a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "%pip install ms-fabric-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e022b8-0d0d-4236-857c-6d253c78122d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "pbi_connection_name = 'fuam pbi-service-api admin'\n",
    "fabric_connection_name = 'fuam fabric-service-api admin'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb77f4b-4cc8-49c6-83cd-231680a1d618",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Import of needed libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414e6feb-ec15-4bb6-b111-5558df98fea0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from zipfile import ZipFile \n",
    "import shutil\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from io import BytesIO\n",
    "import yaml\n",
    "import sempy.fabric as fabric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44471f8d-0f6f-4d86-9bef-20a0afccd13e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Download of source & config files\n",
    "This part downloads all source and config files of FUAM needed for the deployment into the ressources of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bfa16b-c718-48d6-81b1-00f456ccc80d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def download_folder_as_zip(repo_owner, repo_name, output_zip, branch=\"main\", folder_to_extract=\"src\",  remove_folder_prefix = \"\"):\n",
    "    # Construct the URL for the GitHub API to download the repository as a zip file\n",
    "    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/zipball/{branch}\"\n",
    "    \n",
    "    # Make a request to the GitHub API\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Ensure the directory for the output zip file exists\n",
    "    os.makedirs(os.path.dirname(output_zip), exist_ok=True)\n",
    "    \n",
    "    # Create a zip file in memory\n",
    "    with zipfile.ZipFile(BytesIO(response.content)) as zipf:\n",
    "        with zipfile.ZipFile(output_zip, 'w') as output_zipf:\n",
    "            for file_info in zipf.infolist():\n",
    "                parts = file_info.filename.split('/')\n",
    "                if  re.sub(r'^.*?/', '/', file_info.filename).startswith(folder_to_extract): \n",
    "                    # Extract only the specified folder\n",
    "                    file_data = zipf.read(file_info.filename)\n",
    "                    output_zipf.writestr(('/'.join(parts[1:]).replace(remove_folder_prefix, \"\")), file_data)\n",
    "\n",
    "def uncompress_zip_to_folder(zip_path, extract_to):\n",
    "    # Ensure the directory for extraction exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "    \n",
    "    # Uncompress all files from the zip into the specified folder\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    \n",
    "    # Delete the original zip file\n",
    "    os.remove(zip_path)\n",
    "\n",
    "repo_owner = \"Microsoft\"\n",
    "repo_name = \"fabric-toolbox\"\n",
    "branch = \"main\"\n",
    "folder_prefix = \"monitoring/fabric-unified-admin-monitoring\"\n",
    "\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/src/src.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/src\", remove_folder_prefix = f\"{folder_prefix}/\")\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/config/config.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/config\" , remove_folder_prefix = folder_prefix)\n",
    "download_folder_as_zip(repo_owner, repo_name, output_zip = \"./builtin/data/data.zip\", branch = branch, folder_to_extract= f\"/{folder_prefix}/data\" , remove_folder_prefix = folder_prefix)\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/config/config.zip\", extract_to= \"./builtin\")\n",
    "uncompress_zip_to_folder(zip_path = \"./builtin/data/data.zip\", extract_to= \"./builtin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef159d-0f52-43a1-a895-5c3b293ffc61",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "base_path = './builtin/'\n",
    "config_path = os.path.join(base_path, 'config/deployment_config.yaml')\n",
    "\n",
    "with open(config_path, 'r') as file:\n",
    "        config = yaml.safe_load(file)\n",
    "\n",
    "deploy_order_path = os.path.join(base_path, 'config/deployment_order.json')\n",
    "with open(deploy_order_path, 'r') as file:\n",
    "        deployment_order = json.load(file)\n",
    "\n",
    "src_workspace_name = config['workspace']\n",
    "src_pbi_connection = config['connections']['pbi_connection']\n",
    "src_fabric_connection = config['connections']['fabric_connection']\n",
    "\n",
    "semantic_model_connect_to_lakehouse = config['fuam_lakehouse_semantic_models']\n",
    "\n",
    "mapping_table=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b4640-8237-433b-ac46-8986531f28ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Definition of deployment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba6a51-107d-4c10-9a41-5cb2a9e07c27",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "# Set environment parameters for Fabric CLI\n",
    "token = notebookutils.credentials.getToken('pbi')\n",
    "os.environ['FAB_TOKEN'] = token\n",
    "os.environ['FAB_TOKEN_ONELAKE'] = token\n",
    "\n",
    "def run_fab_command( command, capture_output: bool = False, silently_continue: bool = False):\n",
    "    result = subprocess.run([\"fab\", \"-c\", command], capture_output=capture_output, text=True)\n",
    "    if (not(silently_continue) and (result.returncode > 0 or result.stderr)):\n",
    "       raise Exception(f\"Error running fab command. exit_code: '{result.returncode}'; stderr: '{result.stderr}'\")    \n",
    "    if (capture_output): \n",
    "        output = result.stdout.strip()\n",
    "        return output\n",
    "\n",
    "def fab_get_id(name):\n",
    "    id = run_fab_command(f\"get /{trg_workspace_name}.Workspace/{name} -q id\" , capture_output = True, silently_continue= True)\n",
    "    return(id)\n",
    "\n",
    "def get_id_by_name(name):\n",
    "    for it in deployment_order:\n",
    "        if it.get(\"name\") == name:\n",
    "                return it.get(\"fuam_id\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def copy_to_tmp(name):\n",
    "    shutil.rmtree(\"./builtin/tmp\",  ignore_errors=True)\n",
    "    path2zip = \"./builtin/src/src.zip\"\n",
    "    with  ZipFile(path2zip) as archive:\n",
    "        for file in archive.namelist():\n",
    "            if file.startswith(f'src/{name}/'):\n",
    "                archive.extract(file, './builtin/tmp')\n",
    "    return(f\"./builtin/tmp/src/{name}\" )\n",
    "\n",
    "\n",
    "def replace_ids_in_folder(folder_path, mapping_table):\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(('.py', '.json', '.pbir', '.platform', '.ipynb', '.tmdl')) and not file_name.endswith('report.json'):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "                    for mapping in mapping_table:  \n",
    "                        content = content.replace(mapping[\"old_id\"], mapping[\"new_id\"])\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "def get_semantic_model_id(report_folder):\n",
    "    definition_file = os.path.join(report_folder, 'definition.pbir')\n",
    "    if os.path.exists(definition_file):\n",
    "        with open(definition_file, 'r', encoding='utf-8') as file:\n",
    "            content = json.load(file)\n",
    "            semantic_model_id = content.get('datasetReference', {}).get('byConnection', {}).get('pbiModelDatabaseName')\n",
    "            if semantic_model_id:\n",
    "                return semantic_model_id\n",
    "    return None\n",
    "\n",
    "def update_sm_connection_to_fuam_lakehouse(semantic_model_folder):\n",
    "    new_sm_db= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.connectionString\", capture_output = True, silently_continue=True)\n",
    "    new_lakehouse_sql_id= run_fab_command(f\"get /{trg_workspace_name}.Workspace/FUAM_Lakehouse.Lakehouse -q properties.sqlEndpointProperties.id\", capture_output = True, silently_continue=True)\n",
    "        \n",
    "    expressions_file = os.path.join(semantic_model_folder, 'definition', 'expressions.tmdl')\n",
    "    if os.path.exists(expressions_file):\n",
    "        with open(expressions_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "            match = re.search(r'Sql\\.Database\\(\"([^\"]+)\",\\s*\"([^\"]+)\"\\)', content)\n",
    "            if match:\n",
    "                old_sm_db, old_lakehouse_sql_id = match.group(1), match.group(2)\n",
    "                content = content.replace(old_sm_db, new_sm_db).replace(old_lakehouse_sql_id, new_lakehouse_sql_id)\n",
    "                with open(expressions_file, 'w', encoding='utf-8') as file:\n",
    "                    file.write(content)\n",
    "\n",
    "\n",
    "def update_report_definition( path): \n",
    "    semantic_model_id = get_semantic_model_id(path)\n",
    "    definition_path = os.path.join(path, \"definition.pbir\")\n",
    "   \n",
    "    with open(definition_path, \"r\", encoding=\"utf8\") as file:\n",
    "        report_definition = json.load(file)\n",
    "\n",
    "    report_definition[\"datasetReference\"][\"byPath\"] = None\n",
    "\n",
    "    by_connection_obj = {\n",
    "            \"connectionString\": None,\n",
    "            \"pbiServiceModelId\": None,\n",
    "            \"pbiModelVirtualServerName\": \"sobe_wowvirtualserver\",\n",
    "            \"pbiModelDatabaseName\": semantic_model_id,\n",
    "            \"name\": \"EntityDataSource\",\n",
    "            \"connectionType\": \"pbiServiceXmlaStyleLive\",\n",
    "        }\n",
    "\n",
    "    report_definition[\"datasetReference\"][\"byConnection\"] = by_connection_obj\n",
    "\n",
    "    with open(definition_path, \"w\") as file:\n",
    "            json.dump(report_definition, file, indent=4)\n",
    "\n",
    "def print_color(text, state):\n",
    "    red  = '\\033[91m'\n",
    "    yellow = '\\033[93m'  \n",
    "    green = '\\033[92m'   \n",
    "    white = '\\033[0m'  \n",
    "    if state == \"error\":\n",
    "        print(red, text, white)\n",
    "    elif state == \"warning\":\n",
    "        print(yellow, text, white)\n",
    "    elif state == \"success\":\n",
    "        print(green, text, white)\n",
    "    else:\n",
    "        print(\"\", text)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a24f9d-acfc-41a4-ae24-772e5200f74a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Creation of connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328d01d-a5ef-4828-b2cf-dcb0d7482e53",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "def create_or_get_connection(name, baseUrl, audience):\n",
    "    try:\n",
    "        run_fab_command(f\"\"\"create .connections/{name}.connection \n",
    "            -P connectionDetails.type=WebForPipeline,connectionDetails.creationMethod=WebForPipeline.Contents,connectionDetails.parameters.baseUrl={baseUrl},connectionDetails.parameters.audience={audience},credentialDetails.type=Anonymous\"\"\")\n",
    "        print_color(\"New connection created. Enter service principal credentials\", \"success\")\n",
    "    except Exception as ex:\n",
    "        print_color(\"Connection already exists\", \"warning\")\n",
    "\n",
    "    conn_id = run_fab_command(f\"get .connections/{name}.Connection -q id\", silently_continue= True, capture_output= True)\n",
    "    print(\"Connection ID:\" + conn_id)\n",
    "    \n",
    "    \n",
    "    return(conn_id)\n",
    "    \n",
    "conn_pbi_service_api_admin = create_or_get_connection(pbi_connection_name, \"https://api.powerbi.com/v1.0/myorg/admin\", \"https://analysis.windows.net/powerbi/api\" )\n",
    "conn_fabric_service_api_admin = create_or_get_connection(fabric_connection_name, \"https://api.fabric.microsoft.com/v1/admin\", \"\thttps://api.fabric.microsoft.com\" )\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_pbi_connection), \"new_id\": conn_pbi_service_api_admin })\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_fabric_connection), \"new_id\": conn_fabric_service_api_admin })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473b6db-df34-4a72-bd1a-05d5ddefa78e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Get current Workspace\n",
    "This cell gets the current workspace to deploy FUAM automatically inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d54542c-1e34-434b-bada-d73fbe7c2535",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "trg_workspace_id = fabric.get_notebook_workspace_id()\n",
    "res = run_fab_command(f\"api -X get workspaces/{trg_workspace_id}\" , capture_output = True, silently_continue=True)\n",
    "trg_workspace_name = json.loads(res)[\"text\"][\"displayName\"]\n",
    "\n",
    "print(f\"Current workspace: {trg_workspace_name}\")\n",
    "print(f\"Current workspace ID: {trg_workspace_id}\")\n",
    "\n",
    "\n",
    "mapping_table.append({ \"old_id\": get_id_by_name(src_workspace_name + \".Workspace\"), \"new_id\": trg_workspace_id })\n",
    "mapping_table.append({ \"old_id\": \"00000000-0000-0000-0000-000000000000\", \"new_id\": trg_workspace_id })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107b5e53-bb86-4305-b3f2-62d259d8ad67",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "mapping_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8628a42-1d8a-4192-8e75-4754e626fede",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Deployment Logic\n",
    "This part iterates through all the items, gets the respective source code, replaces all IDs dynamically and deploys the new item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ef8af-ff5a-4323-869c-7198e910c0bf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "jupyter_python"
    }
   },
   "outputs": [],
   "source": [
    "exclude = [src_workspace_name + \".Workspace\", src_pbi_connection, src_fabric_connection]\n",
    "\n",
    "for it in deployment_order:\n",
    "\n",
    "    new_id = None\n",
    "    \n",
    "    name = it[\"name\"]\n",
    "    \n",
    "    if name in exclude:\n",
    "            continue\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"#############################################\")\n",
    "    print(f\"Deploying {name}\")\n",
    "\n",
    "    # Copy and replace IDs in the item\n",
    "    tmp_path = copy_to_tmp(name)\n",
    "    replace_ids_in_folder(tmp_path, mapping_table)\n",
    "\n",
    "    cli_parameter = ''\n",
    "    if \"Notebook\" in name:\n",
    "        cli_parameter = cli_parameter + \" --format .ipynb\"\n",
    "    elif \"Lakehouse\" in name:\n",
    "        run_fab_command(f\"create /{trg_workspace_name}.Workspace/{name}\" , silently_continue=True )\n",
    "        new_id = fab_get_id(name)\n",
    "        mapping_table.append({ \"old_id\": get_id_by_name(name), \"new_id\": new_id })\n",
    "        \n",
    "        continue\n",
    "    elif \"Report\" in name:\n",
    "        update_report_definition(  tmp_path  )\n",
    "    elif name in semantic_model_connect_to_lakehouse:\n",
    "        update_sm_connection_to_fuam_lakehouse(tmp_path)\n",
    "    \n",
    "    \n",
    "    run_fab_command(f\"import  /{trg_workspace_name}.Workspace/{name} -i {tmp_path} -f {cli_parameter} \", silently_continue= True)\n",
    "    new_id= fab_get_id(name)\n",
    "    mapping_table.append({ \"old_id\": it[\"fuam_id\"], \"new_id\": new_id })\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "jupyter_kernel_name": "python3.11",
   "name": "jupyter"
  },
  "kernelspec": {
   "display_name": "Jupyter",
   "language": "Jupyter",
   "name": "jupyter"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "jupyter_python",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
